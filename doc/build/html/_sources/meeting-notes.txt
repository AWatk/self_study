.. _meeting-notes:

Meeting Notes
=============


.. note::
   This will be a brief summary of research meetings.  The basic structure will be:

   * Presented Items
   * What was discussed
   * Takeways
   * TODO's

.. contents::
   :depth: 2

November 20, 2017
-----------------

Presented
^^^^^^^^^
:download:`Initial Work <presentations/Initial_Research_Topics_and_Work.pptx>`


Discussed
^^^^^^^^^
Went over current work:
 * ROSMOD
 * HFSM
 * Mobile Robot Test Platform (working name of WALL-E)

Discussed potential research areas:
 * Mobile Robot Motion Planning
 * Holonomic Wheels
 * Human-Robot Interaction
 * Wheelchair-User Dynamics
   
  * Possible pertubations to a nominal controller
  * Develop a robust controller to correct perturbations
    
 * Intent recognition
 * Hybrid Automata (to extend HFSM)

There is also interest in extending user interaction/utility by utilizing control schemes such as the hololens for manipulating the end effector of a robotic arm. I brought up the idea of eventually mounting a small arm onto the top of WALL-E for trial runs.

Takeways
^^^^^^^^

When performing a literature review:
 * Look for survey papers -- The later the best -- and follow up on their references
 * Don't read whole paper
 * Understand what the writers are doing/trying to do
 * Worry about math details later

Lit Review Key Words:
 * Mobile Robot Motion Planning
   
  * Sensor-based
  * Potential Field based
    
 * Holonomnic Wheels and Drives
 * Human-Robot Interaction
   
  * modes of interaction
  * brain-machine interfaces
    
 * Wheelchair-user Dynamics
 * Intent Recognition
   
  * Affective Computing
  * Robotics Intent Recogntion
  * Anticipatory Control Theory

Next meeting will be in January since Sarkar is traveling and I'm still working full time at Max Mobility.     
   
   

Todo
^^^^

* Background Reading/Lit Review


January 19, 2018
----------------

Presented
^^^^

:download:`Meeting Slides <presentations/Meeting-1-19-18.pptx>`

Discussed
^^^^

Addressed the acquisition of Max Mobility by Permobil - Nothing changes on our end.

Met with Paul, was told Max Mobility should have documents from him in two weeks.

Discussed logistics -- lab space, meeting time, etc. Nothing was decided on yet except that meeting in the mornings is preferable. Waiting until the lab meeting is scheduled and will try to schedule my individual meeting for the morning on the same day.

Covered a few key papers gleaned from literature-review-so-far.

Showed the myo-ware armband and intent to replicate the svm paper as a intro to svm ML.

Listed a few textbooks I will be reading/skimming through to help shore up current lack of knowledge.

Talked about how current literature search informs my research path.  Then presented the following roadmap:

#. SLAM
#. Autonomous Path Planning
#. Shared Control
#. Benefits -- Safety/Social/Mental/Perception

Both SLAM and Autonomous Path Planning will simply be getting up to speed and implementing current State of the Art.  They are required as the foundation for any work that will be done. The novelty and research will be in the concept of shared control. Looking into how to properly share control of the mobile device between the user and any onboard autonomous system. This novel research will then be justified by looking at the benefits of such shared control.

Shared Control can be further split into:

#. Intent Recognition
#. Alternative Interface Design
#. Affective State

Benefits include:

#. Cognitive Load
#. Environment Exploration
#. Executive Function
#. ???


Takeaways
^^^^

Suggested courses that will be of greate benefit:

* Random Processes
* Machine Learning

Both are offered next spring.

Lots of possiblities in Intent Recognition in terms of novel devices

Also important to find a way to determine intent implicitly

Todo
^^^^
 
* Continue lit review
* Read textbooks
* Go through SLAM tutorials
* Redo this site's landing page to better reflect the current roadmap

February 16, 2018
-----------------

Presented
^^^^^^^^^

:download:`Meeting Slides <presentations/Meeting-2-16-18.pptx>`

Discussed
^^^^^^^^^

Recapped my application to the Toyota Mobility Unlimited Challenge's Discovery Award for seed funding

Went over the current status of the SLAM mobile robot testbed.

* It moves
* Map building from a single camera using visual odometry
* Currently working on specifying the robot's transform tree so that multiple cameras can be integrated as well as the iCreate's built in odometry

Voiced my concern about not really knowing what to do about steps on the HRI side of things other than read more papers.

Takeaways
^^^^^^^^^

There could be a small contribution in the application of SLAM for powered wheelchairs.

Once the slam platform is up, a path planner based on Dubins path might be feasible

For shared autonomous control based on Activities of Daily Living (ADL):

* Break down the task into a set of primitives
* Identify level of autonomy for each sub-task
* Shared control through composition
* Generates a graph structure where each vertex is a primitive with an associated control scheme and task execution is simply graph traversal

Todo
^^^^

* Read about Dubins Path Plannning
* Send Dr. Sarkar papers on shared control
* Poke Dr. Sarkar for any literature on ADL identification he did for upper arm extremities
* Start brainstorming for ADL identificaiton, decomposition, and execution framework
 * WebGME based tool would be useful for combination of state machines, graph traversal, and the all-important code generation
 * Machine learning hook into identification of ADL primitives

February 23, 2018
-----------------

Presented
^^^^^^^^^

:download:`Meeting Slides <presentations/Meeting-2-23-18.pptx>`


Discussed
^^^^^^^^^

Max Mobility Visit

Discussed shared control and classification papers that were previously emailed

Rehabilitation vs Everyday Use:
It seems there exists a conflict stemming from the objectives for the use of an intelligent system for training vs every day use. Sarkar Visual Error Augmentation paper points out that for training purposes, exacerbation of error leads to better performance in training. For everyday use, the goal is the reduce error and make mobility easier. In other words, while training seeks to heighten the cognitive load and motivate the user, every day use seeks to lower load and make the task as transparent as possible.

ADL Decomposition:
Initial thought was to decompose into primitives associated with the degrees of freedom of the system. Previous work is slightly higher level. Might a better approach be to associate primitives with specific subtasks (move away from an obstacle. Follow a wall, etc.)

Met with Paul, he is going to move the grant work to another assistant (forgot her name) since he doesn't have the bandwidth

Met with Swapnil to discuss his SLAM work and possibly working with Dr. Sarkar this summer. Could possibly do something with my project.

Takeaways
^^^^^^^^^

Nilanjan will get back with me on a visit time.

He suggested reading a paper of his called "A descrete-event systems approach to modeling dextrous manipulation"

A primitive can consist of an Action-Sensing tuple. Sensing inputs from both machine and human.

A semi-autonomous controller can consist of an observer that may not always react

Reactive control - action without deliberation

As touched in the shared control papers discussed, social and quality of life constraints must be included into path planning

User Intention - Gaze is most important. How to find gaze? Simplest way is to model an ellipsoid for the face with eys located. Gaze is the normal vector of the ellipsoid located at the midpoint between the eyes.

TODO
^^^^

* Poke Nilanjan about visit date
* Read "A Discrete-event systems approach to modeling dextrous manipulation"
* Look into path-planning with social constraints
* Inquire about internships

